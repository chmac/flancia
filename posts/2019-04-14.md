<!--
.. title: 9900 hours to go
.. slug: 100
.. date: 2019-04-14 18:43:48 UTC+02:00
.. tags: 
.. category: 
.. link: 
.. description: 
.. type: text
.. status:
-->

Whoa, there go 15 days without posting. It's funny, how many times have I run into random abandoned blogs where the last few posts begin like this post? I wonder if I'll quit anytime soon. I don't feel like quitting really, I've just been busy with other things. Let's wait and see :)

I wrote several things that I thought of posting, then didn't because I felt they needed work. It's funny -- I don't really have any regular readers, I think, so it doesn't make an immediate difference whether I post what I write straight away (and perhaps edit it live) or postpone publishing until I've "polished" it (which realistically may never happen). 

I feel like I mostly write for archive.org; what I write could end up persisting there, and might be read many years from now. This train of thought led me to write one of the pieces I mentioned. I will probably clean it up a bit and post it after finishing this entry (but then again, maybe not).

Going back to the 15 day long hiatus: I've been busy with several things related to Machine Learning -- and, to put it succinctly, that makes me happy. 

I have this fuzzy long-term plan to learn it well, and a more tactical approach that consists in just keeping an eye out for reasonable opportunities to apply what I've learnt hands-on. Now I've finally found a project at work that could benefit from ML, so I've put aside some of my time to experiment with it. Everything is taking long, as it's usually the case with programming (for me, anyway), but I really enjoy the process so for once I don't really mind.  It's such a nice change; I don't really feel this way about my day job, usually. I don't think I've felt this way about something technical since I was in university.

I've also read papers that I found stimulating:

 * "[Language Models are Unsupervised Multitask Learners][1]" -- the GPT-2 paper (really should have read it before writing about GPT-2, shame).
 * "[Attention Is All You Need][2]" -- a.k.a. the Transformer paper.
 * "[Generative Adversarial Nets][3]". I need to keep reading on GANs, a lot of the math went over my head.

I've probably spent 35h over the last three weeks doing ML, and if I remember correctly I've enjoyed all of it. Overall I reckon I've probably put 100 hours into learning ML in all formats (Coursera and podcasts included) since I started.

Going by the [now contended 10000 hours rule of thumb][1] -- I have 9900 hours to go.

[1]: https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf
[2]: https://arxiv.org/abs/1706.03762
[3]: https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf
[4]: https://www.businessinsider.com/new-study-destroys-malcolm-gladwells-10000-rule-2014-7?r=US&IR=T
